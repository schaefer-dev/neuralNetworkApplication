% exercise sheet with header on every page for math or close subjects
\documentclass[12pt]{article}
\usepackage[utf8]{inputenc} 
\usepackage{latexsym} 
\usepackage{multicol}
\usepackage{fancyhdr}
\usepackage{amsfonts} 
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{listings}
\usepackage{graphicx}

% Shortcuts for bb, frak and cal letters
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Pfrak}{\mathfrak{P}}
\newcommand{\Pfrac}{\mathfrak{P}}
\newcommand{\Bfrac}{\mathfrak{P}}
\newcommand{\Bfrak}{\mathfrak{B}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Acal}{\mathcal{A}}

% formating
\topmargin -1.5cm 
\textheight 24cm
\textwidth 16.0 cm 
\oddsidemargin -0.1cm

% Fancy Header on every Page
\pagestyle{fancy}
\lhead{\textbf{Pattern and Speech Recognition}}
\rhead{Daniel Sch√§fer (2549458)\\ Christian Bohnenberger (2548364) \\ Dominik Weber (2548553)}
\renewcommand{\headrulewidth}{1.2pt}

\setlength{\headheight}{45pt} 

\begin{document}
\pagenumbering{gobble}

% TODO set the number of the exercise sheet here!
\setcounter{section}{2}

\subsection{PCA}
\begin{enumerate}[a)]
    \item 
        % TODO
        TODO
    \item
        % TODO
        TODO
    \item
        % TODO
        TODO
\end{enumerate}


\subsection{Numerical Computation 1}

\begin{enumerate}[a)]
    \item 
        % TODO
        As explained in the deeplearning book:\\
        Both of these difficulties can be resolved by instead evaluating $\text{softmax}(z)$ where $z = x - \text{max}_i x_i$. Simple algebra shows that the value of the softmax function is not changed analytically by adding or subtracting a scalar from the input vector. Subtracting $\text{max}_i x_i$ results in the largest argument to exp being 0, which rules out the possibility of overflow. Likewise, at least one term in the denominator has a value of 1, which rules out the possibility of underflow in the denominator leading to a division by zero.
    \item
        % TODO
        As explained in the deeplearning book:\\
        Underflow in the numerator can still cause the expression as a whole to evaluate to zero. This means that if we implement log softmax(x) by first running the softmax subroutine then passing the result to the log function, we could erroneously obtain $-\infty$.
\end{enumerate}


\subsection{Numerical Computation 2}
\begin{enumerate}[a)]
    \item 
        see File
    \item
        % TODO keine Ahnung was man hier gross schreiben soll
        TODO: Justify your answer and explain why the path looks like that.
    \item
        % TODO
        \begin{figure}[H]
            \centering
            \includegraphics[width=300pt]{pictures/figure_1.png}
            \caption{gradient Descent with $\epsilon = 0.1$}
            \label{fig:fig1}
        \end{figure}
        as you can see in Figure \ref{fig:fig1} this is a very interesting special case because with this $\epsilon$ the gradient descent alternates between the ''sides'' (in respect to X) of our surface. This is caused by the derivative which is alternating between the same value positive and negative for the descent in direction X. So we are esentially endlessly looping in X-direction without any progress while only ``progressing'' in Y-direction. For every starting point there is exactly a \underline{single value} of $\epsilon$ which results in this behaviour. This result is caused by the symetry of our surface!
\end{enumerate}


\subsection{Numerical Computation 3}
\begin{enumerate}[a)]
    \item 
        see File
    \item
        % TODO
        TODO: Compare the path of finding the minimum to the one from exercise 3.
\end{enumerate}


\subsection{Numerical Computation 4}
% TODO
TODO: Design a meaningful stopping criterion for stopping the iterations from exercise 3 or 4.


\end{document}
