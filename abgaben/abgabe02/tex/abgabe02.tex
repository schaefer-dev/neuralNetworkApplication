% exercise sheet with header on every page for math or close subjects
\documentclass[12pt]{article}
\usepackage[utf8]{inputenc} 
\usepackage{latexsym} 
\usepackage{multicol}
\usepackage{fancyhdr}
\usepackage{amsfonts} 
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{listings}
\usepackage{graphicx}

% Shortcuts for bb, frak and cal letters
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Pfrak}{\mathfrak{P}}
\newcommand{\Pfrac}{\mathfrak{P}}
\newcommand{\Bfrac}{\mathfrak{P}}
\newcommand{\Bfrak}{\mathfrak{B}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Acal}{\mathcal{A}}

% formating
\topmargin -1.5cm 
\textheight 24cm
\textwidth 16.0 cm 
\oddsidemargin -0.1cm

% Fancy Header on every Page
\pagestyle{fancy}
\lhead{\textbf{Pattern and Speech Recognition}}
\rhead{Daniel Sch√§fer (2549458)\\ Christian Bohnenberger (2548364) \\ Dominik Weber (2548553)}
\renewcommand{\headrulewidth}{1.2pt}

\setlength{\headheight}{45pt} 

\begin{document}
\pagenumbering{gobble}

% TODO set the number of the exercise sheet here!
\setcounter{section}{2}

\subsection{PCA}
\begin{enumerate}[a)]
    \item 
        % TODO
        $$X =\begin{bmatrix} 1&1 \\ 2&2 \\ 3 & 1 \\ 4 & 1 \end{bmatrix}$$\\
        $X$ with mean $0$:\\
        $$X =\begin{bmatrix} -\frac{3}{2}& -\frac{1}{4} \\ -\frac{1}{2}&\frac{3}{4} \\ \frac{1}{2} & -\frac{1}{4} \\ \frac{3}{2} & -\frac{1}{4} \end{bmatrix}$$\\
        We continue with the matrix with mean zero.\\
        $$X^T * X = \begin{bmatrix} -\frac{3}{2}& -\frac{1}{2} & \frac{1}{2} & \frac{3}{2} \\
        				-\frac{1}{4} & \frac{3}{4} & -\frac{1}{4}  & -\frac{1}{4} \end{bmatrix} *
        				\begin{bmatrix} -\frac{3}{2}& -\frac{1}{4} \\ -\frac{1}{2}&\frac{3}{4} \\ \frac{1}{2} & -\frac{1}{4} \\ \frac{3}{2} & -\frac{1}{4} \end{bmatrix} 
        				= \begin{bmatrix}
        					5 & -0.5 \\
        					-0.5 & 3
        				\end{bmatrix}
        				$$\\
        	Now we can compute the eigenvalues:\\
        		$$ (5 - \lambda)*(3 - \lambda)-0.25 =0$$\\
        		$$ 15 - 8\lambda + \lambda^2 -0.25 = 0$$\\
        		$$ \Rightarrow \lambda_1= 2.88197, \lambda_2=5.11803 $$\\
        		
        		$$ \begin{bmatrix} 5-5.11803 & -0.5 \\ -0.5 & 3-5.11803 \end{bmatrix}* \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}
        		=\begin{bmatrix} 0 \\ 0 \end{bmatrix} $$\\
        		$$ \Rightarrow -\frac{1}{2}*x_1 - 2.11803*x_2= 0 $$\\ 
        		$$ \Rightarrow  \text{eigenvector:}  \begin{bmatrix} 4.23606 \\ 1 \end{bmatrix} $$\\
        		$$ D= \begin{bmatrix} 4.23606 \\ 1 \end{bmatrix} $$\\
        		Encoder: $f(x)= D^T * x$\\
        		Decoder: $g(x)= D* D^T *x$\\
        		\begin{itemize}
        			\item $f(x^{(1)})= \begin{bmatrix} 4.23606, & 1 \end{bmatrix} * \begin{bmatrix} 1 \\ 1 \end{bmatrix} = 5.23606$
        			\item $f(x^{(2)})= \begin{bmatrix} 4.23606, & 1 \end{bmatrix} * \begin{bmatrix} 2 \\ 2 \end{bmatrix} = 10.47212$
        			\item $f(x^{(3)})= \begin{bmatrix} 4.23606, & 1 \end{bmatrix} * \begin{bmatrix} 3 \\ 1 \end{bmatrix} = 13.70818$
        			\item $f(x^{(4)})= \begin{bmatrix} 4.23606, & 1 \end{bmatrix} * \begin{bmatrix} 4 \\ 1 \end{bmatrix} = 17.94424$
        		\end{itemize}
    \item
        % TODO
        $$X =\begin{bmatrix} -1&1 \\ -2&2 \\ -1 & 3 \\ -1 & 4 \end{bmatrix}$$\\
        $X$ with mean $0$:\\
        $$X =\begin{bmatrix} \frac{1}{4}& -\frac{3}{2} \\ -\frac{3}{4}&-\frac{1}{2} \\ \frac{1}{4} & \frac{1}{2} \\ \frac{1}{4} & \frac{3}{2} \end{bmatrix}$$\\
        We continue with the matrix with mean zero.\\
        $$X^T * X = \begin{bmatrix} \frac{1}{4} & -\frac{3}{4} & \frac{1}{4}  & \frac{1}{4}\\
        				-\frac{3}{2}& -\frac{1}{2} & \frac{1}{2} & \frac{3}{2} \\
        				\end{bmatrix} *
        				\begin{bmatrix} \frac{1}{4}& -\frac{3}{2} \\ -\frac{3}{4}&-\frac{1}{2} \\ \frac{1}{4} & \frac{1}{2} \\ \frac{1}{4} & \frac{3}{2} \end{bmatrix}
        				= \begin{bmatrix}
        					0.75 & 0.5 \\
        					0.5 & 5
        				\end{bmatrix}
        				$$\\
        	Now we can compute the eigenvalues:\\
        		$$ (0.75 - \lambda)*(5 - \lambda)-0.25 =0$$\\
        		$$ 3.75 - 5.75\lambda + \lambda^2 -0.25 = 0$$\\
        		$$ \Rightarrow \lambda_1= 0.691969, \lambda_2=5.05803 $$\\
        		
        		$$ \begin{bmatrix} 0.75-5.05803 & 0.5 \\ 0.5 & 5-5.05803 \end{bmatrix}* \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}
        		=\begin{bmatrix} 0 \\ 0 \end{bmatrix} $$\\
        		$$ \Rightarrow \frac{1}{2}*x_1 - 0.05803*x_2= 0 $$\\ 
        		$$ \Rightarrow  \text{eigenvector:}  \begin{bmatrix} 0.115288 \\ 0.99332 \end{bmatrix} $$\\
        		$$ D= \begin{bmatrix} 0.115288 \\ 0.99332 \end{bmatrix} $$\\
        		Encoder: $f(x)= D^T * x$\\
        		Decoder: $g(x)= D* D^T *x$\\
        		\begin{itemize}
        			\item $f(x^{(1)})= \begin{bmatrix} 0.115288 \\ 0.99332 \end{bmatrix} * \begin{bmatrix} 1 \\ 1 \end{bmatrix} = 0.878032$
        			\item $f(x^{(2)})= \begin{bmatrix} 0.115288 \\ 0.99332 \end{bmatrix} * \begin{bmatrix} 2 \\ 2 \end{bmatrix} = 1.756064$
        			\item $f(x^{(3)})= \begin{bmatrix} 0.115288 \\ 0.99332 \end{bmatrix} * \begin{bmatrix} 3 \\ 1 \end{bmatrix} = 0.647456$
        			\item $f(x^{(4)})= \begin{bmatrix} 0.115288 \\ 0.99332 \end{bmatrix} * \begin{bmatrix} 4 \\ 1 \end{bmatrix} = 0.532168$
        		\end{itemize}
    \item
        % TODO
        TODO
\end{enumerate}


\subsection{Numerical Computation 1}

\begin{enumerate}[a)]
    \item 
        % TODO
        As explained in the deeplearning book:\\
        Both of these difficulties can be resolved by instead evaluating $\text{softmax}(z)$ where $z = x - \text{max}_i x_i$. Simple algebra shows that the value of the softmax function is not changed analytically by adding or subtracting a scalar from the input vector. Subtracting $\text{max}_i x_i$ results in the largest argument to exp being 0, which rules out the possibility of overflow. Likewise, at least one term in the denominator has a value of 1, which rules out the possibility of underflow in the denominator leading to a division by zero.
    \item
        % TODO
        As explained in the deeplearning book:\\
        Underflow in the numerator can still cause the expression as a whole to evaluate to zero. This means that if we implement log softmax(x) by first running the softmax subroutine then passing the result to the log function, we could erroneously obtain $-\infty$.
\end{enumerate}


\subsection{Numerical Computation 2}
\begin{enumerate}[a)]
    \item 
        see File
    \item
        % TODO keine Ahnung was man hier gross schreiben soll
        \begin{figure}[H]
            \centering
            \includegraphics[width=300pt]{pictures/figure_2.png}
            \caption{function surface perspektive 1}
            \label{fig:fig2}
        \end{figure}
        \begin{figure}[H]
            \centering
            \includegraphics[width=300pt]{pictures/figure_3.png}
            \caption{function surface perspektive 2}
            \label{fig:fig3}
        \end{figure}
        as you can see in Figure \ref{fig:fig2} and \ref{fig:fig3} our Function has the look of an sloap.\\
        \begin{figure}[H]
            \centering
            \includegraphics[width=300pt]{pictures/figure_4.png}
            \caption{gradient descent steps, perspektive 1}
            \label{fig:fig4}
        \end{figure}
        \begin{figure}[H]
            \centering
            \includegraphics[width=300pt]{pictures/figure_5.png}
            \caption{gradient descent steps, perspektive 2}
            \label{fig:fig5}
        \end{figure}
        \begin{figure}[H]
            \centering
            \includegraphics[width=300pt]{pictures/figure_6.png}
            \caption{gradient descent direction}
            \label{fig:fig6}
        \end{figure}
        If we now take a look at the steps of the gradient descent, displayed in Figure \ref{fig:fig4} and \ref{fig:fig5} we can see that we are starting at point $(-2 \vert 4 \vert 84)$. the next descent goes down to $(-0.4 \vert 3.9 \vert 7)$ as shown in \ref{fig:fig6}. 
        
        The descent continues down the slope with decreasing $X$ values fairly quickly towards zero. The same happens for the Y Coordinates but less quickly. 
        
        This is caused by the way the function is being setup. The derivative in $X$ direction is growing much quicker (distancing from $0$) compared to the derivative in $Y$ direction. Thats why we are approaching $X = 0$ much much quicker than we are $Y = 0$. The value for $z$ changes accordingly and also tends towards $0$ just as expected. 
    \item
        % TODO
        \begin{figure}[H]
            \centering
            \includegraphics[width=300pt]{pictures/figure_1.png}
            \caption{gradient Descent with $\epsilon = 0.1$}
            \label{fig:fig1}
        \end{figure}
        as you can see in Figure \ref{fig:fig1} down below this is a very interesting special case because with this $\epsilon$ the gradient descent alternates between the ''sides'' (in respect to X) of our surface. This is caused by the derivative which is alternating between the same value positive and negative for the descent in direction X. So we are esentially endlessly looping in X-direction without any progress while only ``progressing'' in Y-direction. For every starting point there is exactly a \underline{single value} of $\epsilon$ which results in this behaviour. This result is caused by the symetry of our surface!
\end{enumerate}


\subsection{Numerical Computation 3}
\begin{enumerate}[a)]
    \item 
        see File
    \item
        % TODO
        TODO: Compare the path of finding the minimum to the one from exercise 3.
\end{enumerate}


\subsection{Numerical Computation 4}
% TODO
stop descent if one of these conditions hold:
\begin{itemize}
    \item 
        the derivative of f in this point is smaller than a chosen constant 0.0001 for 3 iterations in a row.
    \item
        we also stop in a special case just like in 2.3c by checking if we start alternating between 2 points. We could do this for example by storing the last 4 values for the derivative and checking if 
        $$ \text{dx}_0 - \text{dx}_2 \leq 0.0001 \quad \land \quad \text{dx}_1 - \text{dx}_3 \leq 0.0001 $$
\end{itemize}


\end{document}
